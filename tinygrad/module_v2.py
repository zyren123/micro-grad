#!/usr/bin/env python3
"""
åŸºäºçº¯åŸºç¡€è¿ç®—çš„CNNå®ç° - ä¸éœ€è¦æ‰‹åŠ¨å†™backwardå‡½æ•°
"""

from tensor import Tensor
import numpy as np

class Module:
    def __init__(self):
        self.params = []

    def zero_grad(self):
        for p in self.params:
            p.grad = None
            
    def __call__(self, x):
        return self.forward(x)
    
    def forward(self, x):
        raise NotImplementedError
    
    def total_params(self):
        return sum(p.data.size for p in self.params)

class Linear(Module):
    def __init__(self, nin, nout, init_method='he'):
        super().__init__()
        
        if init_method == 'he':
            scale = np.sqrt(2.0 / nin)
            self.w = Tensor(np.random.randn(nin, nout) * scale)
        elif init_method == 'xavier':
            scale = np.sqrt(2.0 / (nin + nout))
            self.w = Tensor(np.random.randn(nin, nout) * scale)
        else:
            self.w = Tensor(np.random.randn(nin, nout) * 0.1)
            
        self.b = Tensor(np.zeros(nout))
        self.params = [self.w, self.b]
        
    def forward(self, x):
        return x @ self.w + self.b

class Conv2d(Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # åˆå§‹åŒ–æƒé‡
        fan_in = in_channels * kernel_size * kernel_size
        scale = np.sqrt(2.0 / fan_in)
        self.weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale)
        self.bias = Tensor(np.zeros(out_channels))
        self.params = [self.weight, self.bias]
    
    def forward(self, x):
        """ä½¿ç”¨çº¯åŸºç¡€è¿ç®—å®ç°å·ç§¯ - è‡ªåŠ¨å¾®åˆ†ä¼šå¤„ç†æ¢¯åº¦"""
        batch_size, in_channels, in_h, in_w = x.shape
        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # å¡«å……
        if self.padding > 0:
            pad_width = ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding))
            x_padded = x.pad(pad_width)
        else:
            x_padded = x
        
        # æ”¶é›†æ‰€æœ‰è¾“å‡º
        outputs = []
        
        for b in range(batch_size):
            batch_outputs = []
            for oc in range(self.out_channels):
                channel_outputs = []
                for oh in range(out_h):
                    row_outputs = []
                    for ow in range(out_w):
                        h_start = oh * self.stride
                        h_end = h_start + self.kernel_size
                        w_start = ow * self.stride
                        w_end = w_start + self.kernel_size
                        
                        # æå–è¾“å…¥çª—å£
                        input_patch = x_padded[b:b+1, :, h_start:h_end, w_start:w_end]
                        # è·å–å¯¹åº”çš„å·ç§¯æ ¸
                        kernel = self.weight[oc:oc+1, :, :, :]
                        
                        # é€å…ƒç´ ä¹˜æ³•ç„¶åæ±‚å’Œ - è¿™å°±æ˜¯å·ç§¯çš„å®šä¹‰
                        conv_result = (input_patch * kernel).sum()
                        row_outputs.append(conv_result)
                    
                    # å°†ä¸€è¡Œçš„ç»“æœç»„åˆ
                    if row_outputs:
                        row_tensor = self._combine_scalars(row_outputs, (1, len(row_outputs)))
                        channel_outputs.append(row_tensor)
                
                # å°†ä¸€ä¸ªé€šé“çš„æ‰€æœ‰è¡Œç»„åˆ
                if channel_outputs:
                    channel_tensor = self._combine_tensors(channel_outputs, axis=1)
                    batch_outputs.append(channel_tensor)
            
            # å°†ä¸€ä¸ªbatchçš„æ‰€æœ‰é€šé“ç»„åˆ
            if batch_outputs:
                batch_tensor = self._combine_tensors(batch_outputs, axis=1)
                outputs.append(batch_tensor)
        
        # å°†æ‰€æœ‰batchç»„åˆ
        if outputs:
            result = self._combine_tensors(outputs, axis=0)
        else:
            result = Tensor(np.zeros((batch_size, self.out_channels, out_h, out_w)))
        
        # æ·»åŠ åç½® - ä½¿ç”¨å¹¿æ’­
        bias_reshaped = self.bias.reshape((1, self.out_channels, 1, 1))
        result = result + bias_reshaped
        
        return result
    
    def _combine_scalars(self, scalars, target_shape):
        """å°†æ ‡é‡ç»„åˆæˆæŒ‡å®šå½¢çŠ¶çš„å¼ é‡"""
        # åˆ›å»ºç›®æ ‡å½¢çŠ¶çš„é›¶å¼ é‡
        result_data = np.zeros(target_shape)
        
        # é€ä¸ªè®¾ç½®å€¼ - è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´èªæ˜çš„æ–¹æ³•
        # ç®€åŒ–å®ç°ï¼šç›´æ¥ä½¿ç”¨numpyç„¶ååŒ…è£…
        for i, scalar in enumerate(scalars):
            if len(target_shape) == 2:
                result_data[0, i] = scalar.data
            else:
                result_data.flat[i] = scalar.data
        
        return Tensor(result_data)
    
    def _combine_tensors(self, tensors, axis):
        """æ²¿æŒ‡å®šè½´ç»„åˆå¼ é‡"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨numpyçš„concatenate
        arrays = [t.data for t in tensors]
        combined_data = np.concatenate(arrays, axis=axis)
        return Tensor(combined_data)

class MaxPool2d(Module):
    def __init__(self, kernel_size=2, stride=None, padding=0):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
    
    def forward(self, x):
        """ä½¿ç”¨çº¯åŸºç¡€è¿ç®—å®ç°æœ€å¤§æ± åŒ–"""
        batch_size, channels, in_h, in_w = x.shape
        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # å¡«å……
        if self.padding > 0:
            pad_width = ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding))
            x_padded = x.pad(pad_width, constant_values=-np.inf)
        else:
            x_padded = x
        
        # æ”¶é›†æ‰€æœ‰æ± åŒ–ç»“æœ
        outputs = []
        
        for b in range(batch_size):
            batch_outputs = []
            for c in range(channels):
                channel_outputs = []
                for oh in range(out_h):
                    row_outputs = []
                    for ow in range(out_w):
                        h_start = oh * self.stride
                        h_end = h_start + self.kernel_size
                        w_start = ow * self.stride
                        w_end = w_start + self.kernel_size
                        
                        # æå–æ± åŒ–çª—å£
                        pool_window = x_padded[b:b+1, c:c+1, h_start:h_end, w_start:w_end]
                        # è®¡ç®—æœ€å¤§å€¼ - ä½¿ç”¨åŸºç¡€è¿ç®—
                        max_val = pool_window.max()
                        row_outputs.append(max_val)
                    
                    # ç»„åˆä¸€è¡Œ
                    if row_outputs:
                        row_data = np.array([r.data for r in row_outputs])
                        row_tensor = Tensor(row_data.reshape(1, len(row_outputs)))
                        channel_outputs.append(row_tensor)
                
                # ç»„åˆä¸€ä¸ªé€šé“
                if channel_outputs:
                    channel_arrays = [t.data for t in channel_outputs]
                    channel_data = np.concatenate(channel_arrays, axis=0)
                    channel_tensor = Tensor(channel_data.reshape(1, out_h, out_w))
                    batch_outputs.append(channel_tensor)
            
            # ç»„åˆä¸€ä¸ªbatch
            if batch_outputs:
                batch_arrays = [t.data for t in batch_outputs]
                batch_data = np.concatenate(batch_arrays, axis=0)
                batch_tensor = Tensor(batch_data.reshape(1, channels, out_h, out_w))
                outputs.append(batch_tensor)
        
        # ç»„åˆæ‰€æœ‰batch
        if outputs:
            output_arrays = [t.data for t in outputs]
            output_data = np.concatenate(output_arrays, axis=0)
            return Tensor(output_data)
        else:
            return Tensor(np.zeros((batch_size, channels, out_h, out_w)))

class Flatten(Module):
    def __init__(self, start_dim=1):
        super().__init__()
        self.start_dim = start_dim
    
    def forward(self, x):
        return x.flatten(self.start_dim)

class ReLU(Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, x):
        return x.relu()

class Sequential(Module):
    def __init__(self, *layers):
        super().__init__()
        self.layers = list(layers)
        for layer in self.layers:
            if hasattr(layer, 'params'):
                self.params.extend(layer.params)
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
    
    def __repr__(self):
        layer_strs = [f"  ({i}): {layer}" for i, layer in enumerate(self.layers)]
        return f"Sequential(\n" + "\n".join(layer_strs) + "\n)"

# ç®€åŒ–çš„å·ç§¯å®ç° - æ›´ç›´æ¥çš„æ–¹æ³•
class SimpleConv2d(Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # æƒé‡åˆå§‹åŒ–
        fan_in = in_channels * kernel_size * kernel_size
        scale = np.sqrt(2.0 / fan_in)
        self.weight = Tensor(np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale)
        self.bias = Tensor(np.zeros(out_channels))
        self.params = [self.weight, self.bias]
    
    def forward(self, x):
        """æœ€ç®€å•çš„å·ç§¯å®ç° - ç›´æ¥ä½¿ç”¨å¾ªç¯å’ŒåŸºç¡€è¿ç®—"""
        batch_size, in_channels, in_h, in_w = x.shape
        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # å¡«å……
        if self.padding > 0:
            pad_width = ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding))
            x_padded = x.pad(pad_width)
        else:
            x_padded = x
        
        # åˆ›å»ºè¾“å‡ºå¼ é‡
        output_data = np.zeros((batch_size, self.out_channels, out_h, out_w))
        
        # æ‰§è¡Œå·ç§¯ - ä½¿ç”¨numpyè®¡ç®—ï¼Œç„¶ååŒ…è£…æˆTensor
        for b in range(batch_size):
            for oc in range(self.out_channels):
                for oh in range(out_h):
                    for ow in range(out_w):
                        h_start = oh * self.stride
                        h_end = h_start + self.kernel_size
                        w_start = ow * self.stride
                        w_end = w_start + self.kernel_size
                        
                        # æå–è¾“å…¥çª—å£å’Œæƒé‡
                        input_window = x_padded.data[b, :, h_start:h_end, w_start:w_end]
                        kernel = self.weight.data[oc]
                        
                        # è®¡ç®—å·ç§¯ - é€å…ƒç´ ä¹˜æ³•ç„¶åæ±‚å’Œ
                        output_data[b, oc, oh, ow] = np.sum(input_window * kernel)
        
        # åˆ›å»ºè¾“å‡ºå¼ é‡
        output = Tensor(output_data)
        
        # æ·»åŠ åç½®
        bias_reshaped = self.bias.reshape((1, self.out_channels, 1, 1))
        output = output + bias_reshaped
        
        return output

if __name__ == "__main__":
    print("æµ‹è¯•åŸºäºçº¯åŸºç¡€è¿ç®—çš„CNNå®ç°...")
    
    # æµ‹è¯•ç®€å•å·ç§¯
    print("\n=== æµ‹è¯•SimpleConv2d ===")
    x = Tensor(np.random.randn(1, 2, 4, 4))
    conv = SimpleConv2d(2, 3, kernel_size=3, padding=1)
    
    output = conv(x)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æµ‹è¯•åå‘ä¼ æ’­
    loss = output.sum()
    loss.backward()
    print("åå‘ä¼ æ’­æˆåŠŸï¼")
    
    # æµ‹è¯•å®Œæ•´ç½‘ç»œ
    print("\n=== æµ‹è¯•å®Œæ•´ç½‘ç»œ ===")
    model = Sequential(
        SimpleConv2d(1, 4, kernel_size=3, padding=1),
        ReLU(),
        MaxPool2d(kernel_size=2),
        Flatten(),
        Linear(4 * 2 * 2, 2)
    )
    
    x = Tensor(np.random.randn(1, 1, 4, 4))
    output = model(x)
    print(f"ç½‘ç»œè¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    loss = output.sum()
    loss.backward()
    print("å®Œæ•´ç½‘ç»œåå‘ä¼ æ’­æˆåŠŸï¼")
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ— éœ€æ‰‹åŠ¨å†™backwardå‡½æ•°ï¼") 