#!/usr/bin/env python3
"""
æµ‹è¯•CNNå®ç°
"""

import numpy as np
from tensor import Tensor
from module import CNN, Conv2d, MaxPool2d, Flatten, Linear, ReLU, Sequential
from loss import cross_entropy

def test_conv2d():
    """æµ‹è¯•å·ç§¯å±‚"""
    print("=== æµ‹è¯•Conv2då±‚ ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ® (batch_size=2, channels=3, height=4, width=4)
    x = Tensor(np.random.randn(2, 3, 4, 4))
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åˆ›å»ºå·ç§¯å±‚ (3è¾“å…¥é€šé“ -> 8è¾“å‡ºé€šé“, 3x3å·ç§¯æ ¸)
    conv = Conv2d(3, 8, kernel_size=3, padding=1)
    print(f"å·ç§¯å±‚: {conv}")
    print(f"æƒé‡å½¢çŠ¶: {conv.weight.shape}")
    print(f"åç½®å½¢çŠ¶: {conv.bias.shape}")
    
    # å‰å‘ä¼ æ’­
    output = conv(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # åå‘ä¼ æ’­æµ‹è¯•
    loss = output.sum()
    loss.backward()
    
    print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {conv.weight.grad.shape}")
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {conv.bias.grad.shape}")
    print("Conv2dæµ‹è¯•é€šè¿‡ï¼\n")

def test_pooling():
    """æµ‹è¯•æ± åŒ–å±‚"""
    print("=== æµ‹è¯•æ± åŒ–å±‚ ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = Tensor(np.random.randn(2, 4, 8, 8))
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # æµ‹è¯•æœ€å¤§æ± åŒ–
    maxpool = MaxPool2d(kernel_size=2, stride=2)
    max_output = maxpool(x)
    print(f"MaxPoolè¾“å‡ºå½¢çŠ¶: {max_output.shape}")
    
    # åå‘ä¼ æ’­æµ‹è¯•
    loss = max_output.sum()
    loss.backward()
    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print("æ± åŒ–æµ‹è¯•é€šè¿‡ï¼\n")

def test_flatten():
    """æµ‹è¯•å±•å¹³å±‚"""
    print("=== æµ‹è¯•Flattenå±‚ ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = Tensor(np.random.randn(2, 4, 3, 3))
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    flatten = Flatten()
    output = flatten(x)
    print(f"å±•å¹³åå½¢çŠ¶: {output.shape}")
    
    # åå‘ä¼ æ’­æµ‹è¯•
    loss = output.sum()
    loss.backward()
    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print("Flattenæµ‹è¯•é€šè¿‡ï¼\n")

def test_sequential():
    """æµ‹è¯•Sequentialå®¹å™¨"""
    print("=== æµ‹è¯•Sequentialå®¹å™¨ ===")
    
    # åˆ›å»ºç®€å•çš„åºåˆ—æ¨¡å‹
    model = Sequential(
        Conv2d(1, 16, kernel_size=3, padding=1),
        ReLU(),
        MaxPool2d(kernel_size=2),
        Conv2d(16, 32, kernel_size=3, padding=1),
        ReLU(),
        MaxPool2d(kernel_size=2),
        Flatten(),
        Linear(32 * 7 * 7, 10)
    )
    
    print(f"æ¨¡å‹ç»“æ„:\n{model}")
    print(f"æ€»å‚æ•°æ•°é‡: {model.total_params()}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    x = Tensor(np.random.randn(2, 1, 28, 28))  # æ¨¡æ‹ŸMNISTæ•°æ®
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    output = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æµ‹è¯•åå‘ä¼ æ’­
    labels = Tensor([0, 1])
    loss = cross_entropy(output.softmax(), labels)
    print(f"æŸå¤±å€¼: {loss.data}")
    
    loss.backward()
    print("Sequentialæµ‹è¯•é€šè¿‡ï¼\n")

def test_cnn_model():
    """æµ‹è¯•å®Œæ•´çš„CNNæ¨¡å‹"""
    print("=== æµ‹è¯•å®Œæ•´CNNæ¨¡å‹ ===")
    
    # åˆ›å»ºCNNæ¨¡å‹
    model = CNN(num_classes=10, input_channels=1)
    print(f"CNNæ¨¡å‹:\n{model}")
    print(f"æ€»å‚æ•°æ•°é‡: {model.total_params()}")
    
    # åˆ›å»ºæ¨¡æ‹ŸMNISTæ•°æ®
    batch_size = 4
    x = Tensor(np.random.randn(batch_size, 1, 28, 28))
    labels = Tensor(np.random.randint(0, 10, batch_size))
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"æ ‡ç­¾: {labels.data}")
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # åº”ç”¨softmax
    probs = output.softmax()
    print(f"æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶: {probs.shape}")
    print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ: {probs.data[0]}")
    
    # è®¡ç®—æŸå¤±
    loss = cross_entropy(probs, labels)
    print(f"äº¤å‰ç†µæŸå¤±: {loss.data}")
    
    # åå‘ä¼ æ’­
    print("å¼€å§‹åå‘ä¼ æ’­...")
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print("æ£€æŸ¥éƒ¨åˆ†å±‚çš„æ¢¯åº¦:")
    conv1 = model.features.layers[0]  # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
    print(f"ç¬¬ä¸€ä¸ªå·ç§¯å±‚æƒé‡æ¢¯åº¦ç»Ÿè®¡:")
    print(f"  å½¢çŠ¶: {conv1.weight.grad.shape}")
    print(f"  å¹³å‡å€¼: {np.mean(conv1.weight.grad):.6f}")
    print(f"  æ ‡å‡†å·®: {np.std(conv1.weight.grad):.6f}")
    
    print("CNNæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼\n")

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("=== æµ‹è¯•æ¢¯åº¦æµåŠ¨ ===")
    
    # åˆ›å»ºç®€å•çš„CNN
    model = Sequential(
        Conv2d(1, 4, kernel_size=3, padding=1),
        ReLU(),
        MaxPool2d(kernel_size=2),
        Flatten(),
        Linear(4 * 14 * 14, 2)
    )
    
    # è¾“å…¥æ•°æ®
    x = Tensor(np.random.randn(1, 1, 28, 28))
    target = Tensor([1])
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = cross_entropy(output.softmax(), target)
    
    print(f"æŸå¤±å€¼: {loss.data}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¯å±‚æ˜¯å¦éƒ½æœ‰æ¢¯åº¦
    print("æ£€æŸ¥å„å±‚æ¢¯åº¦:")
    for i, layer in enumerate(model.layers):
        if hasattr(layer, 'params') and layer.params:
            for j, param in enumerate(layer.params):
                if param.grad is not None:
                    grad_norm = np.linalg.norm(param.grad)
                    print(f"  å±‚{i} å‚æ•°{j}: æ¢¯åº¦èŒƒæ•° = {grad_norm:.6f}")
                else:
                    print(f"  å±‚{i} å‚æ•°{j}: æ— æ¢¯åº¦!")
    
    print("æ¢¯åº¦æµåŠ¨æµ‹è¯•å®Œæˆï¼\n")

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•CNNå®ç°...\n")
    
    try:
        test_conv2d()
        test_pooling()
        test_flatten()
        test_sequential()
        test_cnn_model()
        test_gradient_flow()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CNNå®ç°æˆåŠŸï¼")
        
    except Exception as e:
        import traceback
        print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__} - {e}")
        traceback.print_exc() 