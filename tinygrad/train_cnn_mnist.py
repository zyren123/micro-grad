#!/usr/bin/env python3
"""
ä½¿ç”¨CNNè®­ç»ƒMNISTæ•°æ®é›†
"""

import numpy as np
import time
from tensor import Tensor
from module import CNN, Sequential, Conv2d, MaxPool2d, Flatten, Linear, ReLU
from loss import cross_entropy

class SimpleOptimizer:
    """ç®€å•çš„SGDä¼˜åŒ–å™¨"""
    def __init__(self, params, lr=0.01):
        self.params = params
        self.lr = lr
    
    def step(self):
        """æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–"""
        for param in self.params:
            if param.grad is not None:
                param.data -= self.lr * param.grad
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        for param in self.params:
            param.grad = None

def generate_mnist_like_data(num_samples=1000, num_classes=10):
    """ç”Ÿæˆç±»ä¼¼MNISTçš„æ¨¡æ‹Ÿæ•°æ®"""
    print(f"ç”Ÿæˆ {num_samples} ä¸ªæ¨¡æ‹ŸMNISTæ ·æœ¬...")
    
    # ç”Ÿæˆéšæœºå›¾åƒæ•°æ® (28x28)
    X = np.random.randn(num_samples, 1, 28, 28) * 0.5
    
    # ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ ä¸€äº›ç‰¹å¾æ¨¡å¼
    y = np.random.randint(0, num_classes, num_samples)
    
    for i in range(num_samples):
        label = y[i]
        # åœ¨å›¾åƒä¸­æ·»åŠ ä¸€äº›ä¸æ ‡ç­¾ç›¸å…³çš„æ¨¡å¼
        # è¿™é‡Œç®€å•åœ°åœ¨ä¸åŒä½ç½®æ·»åŠ äº®ç‚¹
        row = (label // 3) * 8 + 5
        col = (label % 3) * 8 + 5
        X[i, 0, row:row+3, col:col+3] += 2.0
        
        # æ·»åŠ ä¸€äº›å™ªå£°
        X[i] += np.random.randn(1, 28, 28) * 0.1
    
    return X, y

def create_simple_cnn():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„CNNæ¨¡å‹"""
    model = Sequential(
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        Conv2d(1, 16, kernel_size=5, padding=2),  # 28x28 -> 28x28
        ReLU(),
        MaxPool2d(kernel_size=2, stride=2),       # 28x28 -> 14x14
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        Conv2d(16, 32, kernel_size=5, padding=2), # 14x14 -> 14x14
        ReLU(),
        MaxPool2d(kernel_size=2, stride=2),       # 14x14 -> 7x7
        
        # åˆ†ç±»å™¨
        Flatten(),
        Linear(32 * 7 * 7, 128),
        ReLU(),
        Linear(128, 10)
    )
    return model

def train_epoch(model, optimizer, X_train, y_train, batch_size=32):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    num_samples = len(X_train)
    total_loss = 0.0
    correct = 0
    num_batches = 0
    
    # éšæœºæ‰“ä¹±æ•°æ®
    indices = np.random.permutation(num_samples)
    
    for i in range(0, num_samples, batch_size):
        # è·å–æ‰¹æ¬¡æ•°æ®
        batch_indices = indices[i:i+batch_size]
        batch_X = X_train[batch_indices]
        batch_y = y_train[batch_indices]
        
        # è½¬æ¢ä¸ºTensor
        x = Tensor(batch_X)
        labels = Tensor(batch_y)
        
        # å‰å‘ä¼ æ’­
        output = model(x)
        probs = output.softmax()
        
        # è®¡ç®—æŸå¤±
        loss = cross_entropy(probs, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.data
        predictions = np.argmax(probs.data, axis=1)
        correct += np.sum(predictions == batch_y)
        num_batches += 1
        
        # æ‰“å°è¿›åº¦
        if num_batches % 10 == 0:
            print(f"  æ‰¹æ¬¡ {num_batches}, æŸå¤±: {loss.data:.4f}")
    
    avg_loss = total_loss / num_batches
    accuracy = correct / num_samples
    return avg_loss, accuracy

def evaluate(model, X_test, y_test, batch_size=32):
    """è¯„ä¼°æ¨¡å‹"""
    num_samples = len(X_test)
    total_loss = 0.0
    correct = 0
    num_batches = 0
    
    for i in range(0, num_samples, batch_size):
        batch_X = X_test[i:i+batch_size]
        batch_y = y_test[i:i+batch_size]
        
        # è½¬æ¢ä¸ºTensor
        x = Tensor(batch_X)
        labels = Tensor(batch_y)
        
        # å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼‰
        output = model(x)
        probs = output.softmax()
        
        # è®¡ç®—æŸå¤±
        loss = cross_entropy(probs, labels)
        
        # ç»Ÿè®¡
        total_loss += loss.data
        predictions = np.argmax(probs.data, axis=1)
        correct += np.sum(predictions == batch_y)
        num_batches += 1
    
    avg_loss = total_loss / num_batches
    accuracy = correct / num_samples
    return avg_loss, accuracy

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹CNNè®­ç»ƒç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # ç”Ÿæˆæ•°æ®
    print("ğŸ“Š å‡†å¤‡æ•°æ®...")
    X_train, y_train = generate_mnist_like_data(num_samples=800, num_classes=10)
    X_test, y_test = generate_mnist_like_data(num_samples=200, num_classes=10)
    
    print(f"è®­ç»ƒé›†: {X_train.shape}, æ ‡ç­¾: {y_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}, æ ‡ç­¾: {y_test.shape}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = create_simple_cnn()
    print(f"æ¨¡å‹ç»“æ„:\n{model}")
    print(f"æ€»å‚æ•°æ•°é‡: {model.total_params():,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = SimpleOptimizer(model.params, lr=0.001)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 5
    batch_size = 16
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å­¦ä¹ ç‡: {optimizer.lr}")
    print("-" * 50)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        start_time = time.time()
        
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, optimizer, X_train, y_train, batch_size)
        
        # è¯„ä¼°
        test_loss, test_acc = evaluate(model, X_test, y_test, batch_size)
        
        epoch_time = time.time() - start_time
        
        print(f"è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"æµ‹è¯• - æŸå¤±: {test_loss:.4f}, å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"è€—æ—¶: {epoch_time:.2f}ç§’")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“ˆ æœ€ç»ˆç»“æœ:")
    final_test_loss, final_test_acc = evaluate(model, X_test, y_test, batch_size)
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.4f}")
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
    print("\nğŸ” é¢„æµ‹ç¤ºä¾‹:")
    sample_indices = np.random.choice(len(X_test), 5, replace=False)
    for idx in sample_indices:
        x = Tensor(X_test[idx:idx+1])
        output = model(x)
        probs = output.softmax()
        predicted = np.argmax(probs.data)
        actual = y_test[idx]
        confidence = probs.data[0, predicted]
        
        print(f"æ ·æœ¬ {idx}: é¢„æµ‹={predicted}, å®é™…={actual}, ç½®ä¿¡åº¦={confidence:.3f} {'âœ“' if predicted == actual else 'âœ—'}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        import traceback
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__} - {e}")
        traceback.print_exc() 